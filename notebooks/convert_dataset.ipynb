{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.nanoevents import NanoEventsFactory, PFNanoAODSchema\n",
    "PFNanoAODSchema.warn_missing_crossrefs = False\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/eos/cms/store/group/phys_jetmet/dholmber/jec-dnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'dev'\n",
    "\n",
    "in_dir = os.path.join(data_dir, 'raw', dataset)\n",
    "out_dir = os.path.join(data_dir, 'preprocessed', dataset)\n",
    "\n",
    "root_files = glob.glob(os.path.join(in_dir, '*.root'))\n",
    "num_files = len(root_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(out_dir)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nanoaod(path):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore', message='found duplicate branch')\n",
    "        events = NanoEventsFactory.from_root(path, schemaclass=PFNanoAODSchema).events()\n",
    "\n",
    "    jets = events.Jet[(ak.count(events.Jet.matched_gen.pt, axis=1) >= 2)]\n",
    "\n",
    "    sorted_jets = jets[ak.argsort(jets.matched_gen.pt, ascending=False, axis=1)]\n",
    "\n",
    "    leading_jets = ak.concatenate((sorted_jets[:,0], sorted_jets[:,1]), axis=0)\n",
    "\n",
    "    selected_jets = leading_jets[(leading_jets.matched_gen.pt > 30) & (abs(leading_jets.matched_gen.eta) < 5)]\n",
    "\n",
    "    valid_jets = selected_jets[~ak.is_none(selected_jets.matched_gen.pt)]\n",
    "\n",
    "    for field in ['dz', 'dzErr', 'd0', 'd0Err']:\n",
    "        valid_jets = valid_jets[ak.all(valid_jets.constituents.pf[field] != np.inf, axis=1)]\n",
    "\n",
    "    return valid_jets, valid_jets.constituents.pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore', message='found duplicate branch')\n",
    "    events = NanoEventsFactory.from_root(root_files[0], schemaclass=PFNanoAODSchema).events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(jet, pf):\n",
    "    jet['target'] = jet.matched_gen.pt / jet.pt\n",
    "    jet['log_pt'] = np.log(jet.pt)\n",
    "    pf['rel_eta'] = (pf.eta - jet.eta) * np.sign(jet.eta)\n",
    "    pf['rel_pt'] = pf.pt / jet.pt\n",
    "    pf['rel_phi'] = (pf.phi - jet.phi + np.pi) % (2 * np.pi) - np.pi\n",
    "    return jet, pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.01463919  0.04959511  0.01747364  0.04252677 -0.02428002]]\n",
      "\n",
      " [[-0.02592992 -0.03550162  0.045031   -0.04034467  0.03529284]]\n",
      "\n",
      " [[-0.00379109  0.02771599 -0.03886534  0.03927812 -0.04328855]]\n",
      "\n",
      " [[-0.02592992 -0.03550162  0.045031   -0.04034467  0.03529284]]\n",
      "\n",
      " [[-0.00379109  0.02771599 -0.03886534  0.03927812 -0.04328855]]\n",
      "\n",
      " [[ 0.01463919  0.04959511  0.01747364  0.04252677 -0.02428002]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(3, 5))\n",
    "input_array = np.array([0, 1, 2, 1, 2, 0])\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet, pf = read_nanoaod(root_files[0])\n",
    "jet, pf = preprocess(jet, pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(array, categories):\n",
    "    cardinality = len(categories)\n",
    "    category_map = dict(zip(categories, range(cardinality)))\n",
    "    for i, val in enumerate(array):\n",
    "        array[i] = category_map[val]\n",
    "    return np.eye(cardinality)[array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [0, 4, 6, 7]\n",
    "encoded_matrix = one_hot_encode(np.array(jet.puId), categories)\n",
    "encoded_matrix = one_hot_encode(np.array(jet.puId), categories)\n",
    "for i in range(len(categories)):\n",
    "    jet[f'puId_{i}'] = encoded_matrix[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = ak.num(pf)\n",
    "flat_pf = ak.flatten(pf)\n",
    "categories = [-1, 0, 1]\n",
    "encoded_matrix = one_hot_encode(np.array(flat_pf.charge), categories)\n",
    "for i in range(len(categories)):\n",
    "    flat_pf[f'charge_{i}'] = encoded_matrix[:,i]\n",
    "pf = ak.unflatten(flat_pf, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(root_file, parquet_dir):\n",
    "    print(parquet_dir)\n",
    "    \n",
    "    jet, pf = read_nanoaod(root_file)\n",
    "    jet, pf = preprocess(jet, pf)\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(parquet_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    ak.to_parquet(jet, os.path.join(parquet_dir, 'jet.parquet'))\n",
    "    ak.to_parquet(pf, os.path.join(parquet_dir, 'pf.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/eos/cms/store/group/phys_jetmet/dholmber/jec-dnn/preprocessed/dev/1\n",
      "/eos/cms/store/group/phys_jetmet/dholmber/jec-dnn/preprocessed/dev/2\n",
      "/eos/cms/store/group/phys_jetmet/dholmber/jec-dnn/preprocessed/dev/3\n",
      "/eos/cms/store/group/phys_jetmet/dholmber/jec-dnn/preprocessed/dev/4\n",
      "/eos/cms/store/group/phys_jetmet/dholmber/jec-dnn/preprocessed/dev/5\n"
     ]
    }
   ],
   "source": [
    "parquet_dirs = ['/'.join((path, str(index))) for index, path in enumerate(itertools.repeat(out_dir, num_files), start=1)]\n",
    "for i in range(len(root_files)):\n",
    "    create_dataset(root_files[i], parquet_dirs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with ProcessPoolExecutor(max_workers=None) as executor:\n",
    "#     parquet_dirs = ['/'.join((path, str(index))) for index, path in enumerate(itertools.repeat(out_dir, num_files), start=1)]\n",
    "#     results = executor.map(create_dataset, root_files, parquet_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## cheers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "gitlab-registry.cern.ch/dholmber/jec-dnn:latest",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
