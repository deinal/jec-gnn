{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Jet Energy Regression with ParticleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Activation, Add, BatchNormalization, Conv2D, Dense, Dropout, Layer, Multiply, Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data location and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "block:user_specified_info"
    ]
   },
   "outputs": [],
   "source": [
    "# parquet_dir = os.path.join('/eos/cms/store/group/phys_jetmet/dholmber/jec-dnn/preprocessed/dev')\n",
    "\n",
    "parquet_dir = '/ssd-home/hdaniel/lab/jec-dnn/data/dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "pipeline-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 512\n",
    "loss = 'mean_absolute_error'\n",
    "optimizer = 'adam'\n",
    "lr = 1.0e-3\n",
    "shortcut = False\n",
    "dropout = 0\n",
    "activation = 'relu'\n",
    "initializer = 'he_normal'\n",
    "pooling = 'average' # average or max\n",
    "batch_norm = False\n",
    "\n",
    "K = 16\n",
    "\n",
    "num_conv_layers = 3\n",
    "num_channels = 3\n",
    "first_channel = 64\n",
    "channel_scale = 2 \n",
    "num_dense_layers = 3\n",
    "first_layer = 256\n",
    "unit_scale = 0.5\n",
    "\n",
    "shuffle_buffer = 64\n",
    "\n",
    "train_size = 0.6\n",
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "\n",
    "num_points = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "jet_numerical = ['log_pt', 'eta', 'mass', 'phi', 'area', 'qgl_axis2', 'qgl_ptD', 'qgl_mult']\n",
    "pf_numerical = ['rel_pt', 'rel_eta', 'rel_phi', 'd0', 'dz', 'd0Err', 'dzErr', 'trkChi2', 'vtxChi2', 'puppiWeight', 'puppiWeightNoLep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "block:get_metadata",
     "prev:user_specified_info"
    ]
   },
   "outputs": [],
   "source": [
    "categorical_map = {\n",
    "    'jet': {\n",
    "        'partonFlavour': [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 21]\n",
    "    },\n",
    "    'pf': {\n",
    "        'charge': [-1, 0, 1],\n",
    "        'lostInnerHits': [-1, 0, 1, 2],\n",
    "        'pdgId': [-211, -13, -11, 1, 2, 11, 13, 22, 130, 211],\n",
    "        'pvAssocQuality': [0, 1, 4, 5, 6, 7],\n",
    "        'trkQuality': [0, 1, 5]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_categorical = []\n",
    "for key, categories in categorical_map['jet'].items():\n",
    "    jet_categorical.extend([f'{key}_{i}' for i in range(len(categories))])\n",
    "    \n",
    "pf_categorical = []\n",
    "for key, categories in categorical_map['pf'].items():\n",
    "    pf_categorical.extend([f'{key}_{i}' for i in range(len(categories))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_fields = jet_numerical + jet_categorical\n",
    "pf_fields = pf_numerical + pf_categorical\n",
    "\n",
    "jet_keys = [f'jet_{field}' for field in jet_fields]\n",
    "pf_keys = [f'pf_{field}' for field in pf_fields]\n",
    "\n",
    "num_jet = len(jet_keys)\n",
    "num_pf = len(pf_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = glob.glob(os.path.join(parquet_dir, '*'))\n",
    "num_dirs = len(dirs)\n",
    "train_split = int(train_size * num_dirs)\n",
    "test_split = int(test_size * num_dirs) + train_split\n",
    "\n",
    "train_dirs = dirs[:train_split]\n",
    "test_dirs = dirs[train_split:test_split]\n",
    "val_dirs = dirs[test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/ssd-home/hdaniel/lab/jec-dnn/data/dev/3',\n",
       " '/ssd-home/hdaniel/lab/jec-dnn/data/dev/4',\n",
       " '/ssd-home/hdaniel/lab/jec-dnn/data/dev/1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet(path):\n",
    "    path = path.decode()\n",
    "\n",
    "def calculate_factors(num_layers, scale):\n",
    "    factors = [1]\n",
    "    for i in range(num_layers - 1):\n",
    "        factors.append(scale * factors[i])\n",
    "    return factors\n",
    "\n",
    "    for field in pf_fields:\n",
    "        none_padded_pf = ak.pad_none(pf[field], target=num_points, clip=True, axis=1)\n",
    "        zero_padded_pf = ak.to_numpy(none_padded_pf).filled(0)\n",
    "        data.append(zero_padded_pf.astype(np.float32))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_wrapper(path):\n",
    "    inp = [path]\n",
    "    Tout = [tf.int32] + [tf.float32] + [tf.float32] * num_jet + [tf.float32] * num_pf\n",
    "    \n",
    "    cols = tf.numpy_function(read_parquet, inp=inp, Tout=Tout)\n",
    "    \n",
    "    keys = ['row_lengths'] + ['target'] + jet_keys + pf_keys\n",
    "    data = {key: value for key, value in zip(keys, cols)}\n",
    "    \n",
    "    data['target'].set_shape((None,))\n",
    "    \n",
    "    row_lengths = data.pop('row_lengths')\n",
    "    row_lengths.set_shape((None,))\n",
    "    \n",
    "    for key in jet_keys:\n",
    "        # Shape from <unknown> to (None,)\n",
    "        data[key].set_shape((None,))\n",
    "        # Shape from (None,) to (None, 1)\n",
    "        data[key] = tf.expand_dims(data[key], axis=1)\n",
    "    \n",
    "    for key in pf_keys:\n",
    "        # Shape from <unknown> to (None, P)\n",
    "        data[key].set_shape((None, num_points))\n",
    "        # Shape from (None, P) to (None, P, 1)\n",
    "        data[key] = tf.expand_dims(data[key], axis=2)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_record(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, features=all_features)\n",
    "\n",
    "def select_features(batch):\n",
    "    jet_data = tf.stack([batch[key] for key in jet_keys], axis=1)\n",
    "    pf_data = tf.stack([batch[key].values for key in pf_keys], axis=1)\n",
    "    pf_data = tf.RaggedTensor.from_row_lengths(pf_data, row_lengths=batch['row_lengths']).to_tensor(shape=(None, num_points, num_pf))\n",
    "    \n",
    "    mask = tf.cast(tf.math.not_equal(pf_data[:,:,0:1], 0), dtype=tf.float32) # 1 if valid\n",
    "    coord_shift = tf.multiply(1e6, tf.cast(tf.math.equal(mask, 0), dtype=tf.float32))\n",
    "    points = tf.concat([pf_data[:,:,1:2], pf_data[:,:,2:3]], axis=2)\n",
    "    \n",
    "    inputs = (pf_data, jet_data, data['points'], data['coord_shift'], data['mask'])\n",
    "    return inputs, data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(paths):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    ds = ds.map(read_parquet_wrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.map(prepare_pf_inputs, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.unbatch().batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_dataset(train_files).shuffle(shuffle_buffer)\n",
    "val_ds = create_dataset(val_files)\n",
    "test_ds = create_dataset(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Activation, Add, BatchNormalization, Conv2D, Dense, Dropout, Layer, Multiply, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mean(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.reduce_mean(inputs, axis=self.axis)\n",
    "\n",
    "\n",
    "class Max(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.reduce_max(inputs, axis=self.axis)\n",
    "\n",
    "\n",
    "class Expand(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.expand_dims(inputs, axis=self.axis)\n",
    "\n",
    "\n",
    "class Squeeze(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.squeeze(inputs, axis=self.axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_particle_net():\n",
    "    \"\"\"\n",
    "    ParticleNet: Jet Tagging via Particle Clouds\n",
    "    arxiv.org/abs/1902.08570\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shapes : dict\n",
    "        The shapes of each input (`points`, `features`, `mask`).\n",
    "    \"\"\"\n",
    "\n",
    "    features = Input(name='features', shape=(num_points, num_pf))\n",
    "    globals = Input(name='globals', shape=(num_jet,))\n",
    "    points = Input(name='points', shape=(num_points, 2))\n",
    "    coord_shift = Input(name='coord_shift', shape=(num_points, 1))\n",
    "    mask = Input(name='mask', shape=(num_points, 1))\n",
    "\n",
    "    outputs = particle_net_base(points, features, mask, coord_shift, globals)\n",
    "\n",
    "    model = Model(inputs=[features, globals, points, coord_shift, mask], outputs=outputs)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def particle_net_base(points, features, mask, coord_shift, globals):\n",
    "    \"\"\"\n",
    "    points : (N, P, C_coord)\n",
    "    features:  (N, P, C_features), optional\n",
    "    mask: (N, P, 1), optional\n",
    "    \"\"\"\n",
    "\n",
    "    fts = features\n",
    "    for layer_idx, sub_channels in enumerate(channels, start=1):\n",
    "        pts = Add(name=f'add_{layer_idx}')([coord_shift, points]) if layer_idx == 1 else Add(name=f'add_{layer_idx}')([coord_shift, fts])\n",
    "        fts = edge_conv(\n",
    "            pts, fts, num_points, sub_channels, name=f'edge_conv_{layer_idx}'\n",
    "        )\n",
    "\n",
    "    fts = Multiply()([fts, mask])\n",
    "\n",
    "    pool = Mean(axis=1)(fts) # (N, C)\n",
    "\n",
    "    x = Concatenate(name='head')([pool, globals])\n",
    "\n",
    "    for layer_idx, n in enumerate(units):\n",
    "        x = Dense(n)(x)\n",
    "        x = Activation(activation)(x)\n",
    "        if dropout:\n",
    "            x = Dropout(dropout)(x)\n",
    "    out = Dense(1, name='out')(x)\n",
    "    return out # (N, num_classes)\n",
    "\n",
    "\n",
    "def edge_conv(points, features, num_points, sub_channels, name):\n",
    "    \"\"\"EdgeConv\n",
    "    Args:\n",
    "        K: int, number of neighbors\n",
    "        in_channels: # of input channels\n",
    "        channels: tuple of output channels\n",
    "        pooling: pooling method ('max' or 'average')\n",
    "    Inputs:\n",
    "        points: (N, P, C_p)\n",
    "        features: (N, P, C_0)\n",
    "    Returns:\n",
    "        transformed points: (N, P, C_out), C_out = channels[-1]\n",
    "    \"\"\"\n",
    "\n",
    "    fts = features\n",
    "    knn_fts = KNearestNeighbors(num_points, K, name=f'{name}_knn')([points, fts])\n",
    "\n",
    "    x = knn_fts\n",
    "    for idx, channel in enumerate(sub_channels, start=1):\n",
    "        x = Conv2D(\n",
    "            channel, kernel_size=(1, 1), strides=1, data_format='channels_last',\n",
    "            use_bias=False if batch_norm else True, kernel_initializer=initializer, name=f'{name}_conv_{idx}'\n",
    "        )(x)\n",
    "        if batch_norm:\n",
    "            x = BatchNormalization(name=f'{name}_batchnorm_{idx}')(x)\n",
    "        if activation:\n",
    "            x = Activation(activation, name=f'{name}_activation_{idx}')(x)\n",
    "\n",
    "    if pooling == 'max':\n",
    "        fts = Max(axis=2, name=f'{name}_max')(x) # (N, P, C')\n",
    "    else:\n",
    "        fts = Mean(axis=2, name=f'{name}_mean')(x) # (N, P, C')\n",
    "\n",
    "    if shortcut:\n",
    "        sc = Expand(axis=2, name=f'{name}_shortcut_expand')(features)\n",
    "        sc = Conv2D(\n",
    "            sub_channels[-1], kernel_size=(1, 1), strides=1, data_format='channels_last',\n",
    "            use_bias=False if batch_norm else True, kernel_initializer=initializer, name=f'{name}_shortcut_conv'\n",
    "        )(sc)\n",
    "        if batch_norm:\n",
    "            sc = BatchNormalization(name=f'{name}_shortcut_batchnorm')(sc)\n",
    "        sc = Squeeze(axis=2, name=f'{name}_shortcut_squeeze')(sc)\n",
    "\n",
    "        x = Add(name=f'{name}_add')([sc, fts])\n",
    "    else:\n",
    "        x = fts\n",
    "\n",
    "    return Activation(activation, name=f'{name}_activation')(x) # (N, P, C')\n",
    "\n",
    "\n",
    "class KNearestNeighbors(Layer):\n",
    "    def __init__(self, num_points, k, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_points = num_points\n",
    "        self.k = k\n",
    "\n",
    "    def call(self, inputs):\n",
    "        points, features = inputs\n",
    "        # distance\n",
    "        D = batch_distance_matrix_general(points, points) # (N, P, P)\n",
    "        _, top_k_indices = tf.math.top_k(-D, k=self.k + 1) # (N, P, K+1)\n",
    "        top_k_indices = top_k_indices[:, :, 1:] # (N, P, K)\n",
    "\n",
    "        queries_shape = tf.shape(features)\n",
    "        batch_size = queries_shape[0]\n",
    "        batch_indices = tf.tile(tf.reshape(tf.range(batch_size), (-1, 1, 1, 1)), (1, self.num_points, self.k, 1))\n",
    "        indices = tf.concat([batch_indices, tf.expand_dims(top_k_indices, axis=3)], axis=3) # (N, P, K, 2)\n",
    "        \n",
    "        knn_fts =  tf.gather_nd(features, indices) # (N, P, K, C)\n",
    "        knn_fts_center = tf.tile(tf.expand_dims(features, axis=2), (1, 1, self.k, 1)) # (N, P, K, C)\n",
    "\n",
    "        return tf.concat([knn_fts_center, tf.subtract(knn_fts, knn_fts_center)], axis=-1) # (N, P, K, 2*C)\n",
    "\n",
    "\n",
    "# A shape is (N, P_A, C), B shape is (N, P_B, C)\n",
    "# D shape is (N, P_A, P_B)\n",
    "def batch_distance_matrix_general(A, B):\n",
    "    r_A = tf.math.reduce_sum(A * A, axis=2, keepdims=True)\n",
    "    r_B = tf.math.reduce_sum(B * B, axis=2, keepdims=True)\n",
    "    m = tf.linalg.matmul(A, tf.transpose(B, perm=(0, 2, 1)))\n",
    "    D = r_A - 2 * m + tf.transpose(r_B, perm=(0, 2, 1))\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "coord_shift (InputLayer)        [(None, 100, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "points (InputLayer)             [(None, 100, 2)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 100, 2)       0           coord_shift[0][0]                \n",
      "                                                                 points[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "features (InputLayer)           [(None, 100, 37)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_1_knn (KNearestNeighb (None, 100, 16, 74)  0           add_1[0][0]                      \n",
      "                                                                 features[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_1_conv_1 (Conv2D)     (None, 100, 16, 64)  4800        edge_conv_1_knn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_1_activation_1 (Activ (None, 100, 16, 64)  0           edge_conv_1_conv_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_1_conv_2 (Conv2D)     (None, 100, 16, 64)  4160        edge_conv_1_activation_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_1_activation_2 (Activ (None, 100, 16, 64)  0           edge_conv_1_conv_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_1_conv_3 (Conv2D)     (None, 100, 16, 64)  4160        edge_conv_1_activation_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_1_activation_3 (Activ (None, 100, 16, 64)  0           edge_conv_1_conv_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_1_mean (Mean)         (None, 100, 64)      0           edge_conv_1_activation_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_1_activation (Activat (None, 100, 64)      0           edge_conv_1_mean[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 100, 64)      0           coord_shift[0][0]                \n",
      "                                                                 edge_conv_1_activation[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_2_knn (KNearestNeighb (None, 100, 16, 128) 0           add_2[0][0]                      \n",
      "                                                                 edge_conv_1_activation[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_2_conv_1 (Conv2D)     (None, 100, 16, 128) 16512       edge_conv_2_knn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_2_activation_1 (Activ (None, 100, 16, 128) 0           edge_conv_2_conv_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_2_conv_2 (Conv2D)     (None, 100, 16, 128) 16512       edge_conv_2_activation_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_2_activation_2 (Activ (None, 100, 16, 128) 0           edge_conv_2_conv_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_2_conv_3 (Conv2D)     (None, 100, 16, 128) 16512       edge_conv_2_activation_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_2_activation_3 (Activ (None, 100, 16, 128) 0           edge_conv_2_conv_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_2_mean (Mean)         (None, 100, 128)     0           edge_conv_2_activation_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_2_activation (Activat (None, 100, 128)     0           edge_conv_2_mean[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 100, 128)     0           coord_shift[0][0]                \n",
      "                                                                 edge_conv_2_activation[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_3_knn (KNearestNeighb (None, 100, 16, 256) 0           add_3[0][0]                      \n",
      "                                                                 edge_conv_2_activation[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_3_conv_1 (Conv2D)     (None, 100, 16, 256) 65792       edge_conv_3_knn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_3_activation_1 (Activ (None, 100, 16, 256) 0           edge_conv_3_conv_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_3_conv_2 (Conv2D)     (None, 100, 16, 256) 65792       edge_conv_3_activation_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_3_activation_2 (Activ (None, 100, 16, 256) 0           edge_conv_3_conv_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_3_conv_3 (Conv2D)     (None, 100, 16, 256) 65792       edge_conv_3_activation_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_3_activation_3 (Activ (None, 100, 16, 256) 0           edge_conv_3_conv_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_3_mean (Mean)         (None, 100, 256)     0           edge_conv_3_activation_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "edge_conv_3_activation (Activat (None, 100, 256)     0           edge_conv_3_mean[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mask (InputLayer)               [(None, 100, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 100, 256)     0           edge_conv_3_activation[0][0]     \n",
      "                                                                 mask[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "mean (Mean)                     (None, 256)          0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "globals (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "head (Concatenate)              (None, 276)          0           mean[0][0]                       \n",
      "                                                                 globals[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          35456       head[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 1)            129         activation_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 312,129\n",
      "Trainable params: 312,129\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=0.001>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn = get_particle_net()\n",
    "dnn.compile(optimizer=optimizer, loss=loss)\n",
    "dnn.optimizer.lr.assign(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(dnn, dpi=100, show_shapes=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    # Reduce learning rate when nearing convergence\n",
    "    reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.2, patience=5, min_lr=1.0e-8,\n",
    "        mode='auto', min_delta=1.0e-4, cooldown=0, verbose=1\n",
    "    )\n",
    "    # Stop early if the network stops improving\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', min_delta=1.0e-4, \n",
    "        patience=7, mode='auto', baseline=None, \n",
    "        restore_best_weights=True, verbose=1\n",
    "    )\n",
    "\n",
    "    return [reduce_lr_on_plateau, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2/2 [==============================] - 4s 922ms/step - loss: 2.0169 - val_loss: 1.1221\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 3s 553ms/step - loss: 1.1859 - val_loss: 0.7635\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 3s 552ms/step - loss: 0.9834 - val_loss: 0.3966\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.4978 - val_loss: 0.5362\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 3s 3s/step - loss: 0.4616 - val_loss: 0.5181\n"
     ]
    }
   ],
   "source": [
    "fit = dnn.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=get_callbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "block:model_evalutaion",
     "prev:train_model"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4/Unknown - 1s 164ms/step - loss: 0.1318"
     ]
    }
   ],
   "source": [
    "result = dnn.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "pipeline-metrics",
     "prev:train_model"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13182656280696392\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "gitlab-registry.cern.ch/ai-ml/kubeflow_images/tensorflow-notebook-gpu-2.1.0:v0.6.1-30",
   "experiment": {
    "id": "new",
    "name": "jec-dnn"
   },
   "experiment_name": "jec-dnn",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "random",
     "algorithmSettings": [
      {
       "name": "random_state",
       "value": "10"
      },
      {
       "name": "acq_optimizer",
       "value": "auto"
      },
      {
       "name": "acq_func",
       "value": "gp_hedge"
      },
      {
       "name": "n_initial_points",
       "value": "10"
      },
      {
       "name": "base_estimator",
       "value": "GP"
      }
     ]
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "additionalMetricNames": [],
     "objectiveMetricName": "result",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": true,
   "pipeline_description": "Jet energy regression using particle net",
   "pipeline_name": "particle-net",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
