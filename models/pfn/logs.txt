$ python train.py -i data/shards -c config.yaml -o results/pfn --gpus 0 1 --save-model --save-weights
GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]
Model: "dnn"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
charged_constituents (InputLaye [(None, None, 25)]   0                                            
__________________________________________________________________________________________________
neutral_constituents (InputLaye [(None, None, 11)]   0                                            
__________________________________________________________________________________________________
secondary_vertices (InputLayer) [(None, None, 10)]   0                                            
__________________________________________________________________________________________________
ch_time_distributed (TimeDistri (None, None, 256)    44800       charged_constituents[0][0]       
__________________________________________________________________________________________________
ne_time_distributed (TimeDistri (None, None, 256)    43904       neutral_constituents[0][0]       
__________________________________________________________________________________________________
sv_time_distributed (TimeDistri (None, None, 128)    11680       secondary_vertices[0][0]         
__________________________________________________________________________________________________
ch_head (Sum)                   (None, 256)          0           ch_time_distributed[0][0]        
__________________________________________________________________________________________________
ne_head (Sum)                   (None, 256)          0           ne_time_distributed[0][0]        
__________________________________________________________________________________________________
sv_head (Sum)                   (None, 128)          0           sv_time_distributed[0][0]        
__________________________________________________________________________________________________
globals (InputLayer)            [(None, 11)]         0                                            
__________________________________________________________________________________________________
head (Concatenate)              (None, 651)          0           ch_head[0][0]                    
                                                                 ne_head[0][0]                    
                                                                 sv_head[0][0]                    
                                                                 globals[0][0]                    
__________________________________________________________________________________________________
head_dense_1 (Dense)            (None, 1024)         667648      head[0][0]                       
__________________________________________________________________________________________________
head_batch_normalization_1 (Bat (None, 1024)         4096        head_dense_1[0][0]               
__________________________________________________________________________________________________
head_activation_1 (Activation)  (None, 1024)         0           head_batch_normalization_1[0][0] 
__________________________________________________________________________________________________
head_dense_2 (Dense)            (None, 512)          524800      head_activation_1[0][0]          
__________________________________________________________________________________________________
head_batch_normalization_2 (Bat (None, 512)          2048        head_dense_2[0][0]               
__________________________________________________________________________________________________
head_activation_2 (Activation)  (None, 512)          0           head_batch_normalization_2[0][0] 
__________________________________________________________________________________________________
head_dense_3 (Dense)            (None, 256)          131328      head_activation_2[0][0]          
__________________________________________________________________________________________________
head_batch_normalization_3 (Bat (None, 256)          1024        head_dense_3[0][0]               
__________________________________________________________________________________________________
head_activation_3 (Activation)  (None, 256)          0           head_batch_normalization_3[0][0] 
__________________________________________________________________________________________________
head_dense_4 (Dense)            (None, 128)          32896       head_activation_3[0][0]          
__________________________________________________________________________________________________
head_batch_normalization_4 (Bat (None, 128)          512         head_dense_4[0][0]               
__________________________________________________________________________________________________
head_activation_4 (Activation)  (None, 128)          0           head_batch_normalization_4[0][0] 
__________________________________________________________________________________________________
head_dense_5 (Dense)            (None, 64)           8256        head_activation_4[0][0]          
__________________________________________________________________________________________________
head_batch_normalization_5 (Bat (None, 64)           256         head_dense_5[0][0]               
__________________________________________________________________________________________________
head_activation_5 (Activation)  (None, 64)           0           head_batch_normalization_5[0][0] 
__________________________________________________________________________________________________
output (Dense)                  (None, 1)            65          head_activation_5[0][0]          
==================================================================================================
Total params: 1,473,313
Trainable params: 1,467,105
Non-trainable params: 6,208
__________________________________________________________________________________________________
Model: "ch_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
ch_slice (InputLayer)        [(None, 25)]              0         
_________________________________________________________________
ch_dense_1 (Dense)           (None, 64)                1664      
_________________________________________________________________
ch_batch_normalization_1 (Ba (None, 64)                256       
_________________________________________________________________
ch_activation_1 (Activation) (None, 64)                0         
_________________________________________________________________
ch_dense_2 (Dense)           (None, 128)               8320      
_________________________________________________________________
ch_batch_normalization_2 (Ba (None, 128)               512       
_________________________________________________________________
ch_activation_2 (Activation) (None, 128)               0         
_________________________________________________________________
ch_dense_3 (Dense)           (None, 256)               33024     
_________________________________________________________________
ch_batch_normalization_3 (Ba (None, 256)               1024      
_________________________________________________________________
ch_activation_3 (Activation) (None, 256)               0         
=================================================================
Total params: 44,800
Trainable params: 43,904
Non-trainable params: 896
_________________________________________________________________
Model: "ne_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
ne_slice (InputLayer)        [(None, 11)]              0         
_________________________________________________________________
ne_dense_1 (Dense)           (None, 64)                768       
_________________________________________________________________
ne_batch_normalization_1 (Ba (None, 64)                256       
_________________________________________________________________
ne_activation_1 (Activation) (None, 64)                0         
_________________________________________________________________
ne_dense_2 (Dense)           (None, 128)               8320      
_________________________________________________________________
ne_batch_normalization_2 (Ba (None, 128)               512       
_________________________________________________________________
ne_activation_2 (Activation) (None, 128)               0         
_________________________________________________________________
ne_dense_3 (Dense)           (None, 256)               33024     
_________________________________________________________________
ne_batch_normalization_3 (Ba (None, 256)               1024      
_________________________________________________________________
ne_activation_3 (Activation) (None, 256)               0         
=================================================================
Total params: 43,904
Trainable params: 43,008
Non-trainable params: 896
_________________________________________________________________
Model: "sv_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
sv_slice (InputLayer)        [(None, 10)]              0         
_________________________________________________________________
sv_dense_1 (Dense)           (None, 32)                352       
_________________________________________________________________
sv_batch_normalization_1 (Ba (None, 32)                128       
_________________________________________________________________
sv_activation_1 (Activation) (None, 32)                0         
_________________________________________________________________
sv_dense_2 (Dense)           (None, 64)                2112      
_________________________________________________________________
sv_batch_normalization_2 (Ba (None, 64)                256       
_________________________________________________________________
sv_activation_2 (Activation) (None, 64)                0         
_________________________________________________________________
sv_dense_3 (Dense)           (None, 128)               8320      
_________________________________________________________________
sv_batch_normalization_3 (Ba (None, 128)               512       
_________________________________________________________________
sv_activation_3 (Activation) (None, 128)               0         
=================================================================
Total params: 11,680
Trainable params: 11,232
Non-trainable params: 448
_________________________________________________________________
Epoch 1/100
8075/8075 [==============================] - 448s 52ms/step - loss: 0.0898 - val_loss: 0.0846
Epoch 2/100
8075/8075 [==============================] - 438s 52ms/step - loss: 0.0826 - val_loss: 0.0814
Epoch 3/100
8075/8075 [==============================] - 438s 53ms/step - loss: 0.0814 - val_loss: 0.0838
Epoch 4/100
8075/8075 [==============================] - 439s 53ms/step - loss: 0.0808 - val_loss: 0.0822
Epoch 5/100
8075/8075 [==============================] - 437s 52ms/step - loss: 0.0805 - val_loss: 0.0803
Epoch 6/100
8075/8075 [==============================] - 439s 52ms/step - loss: 0.0803 - val_loss: 0.0800
Epoch 7/100
8075/8075 [==============================] - 439s 52ms/step - loss: 0.0801 - val_loss: 0.0805
Epoch 8/100
8075/8075 [==============================] - 440s 52ms/step - loss: 0.0799 - val_loss: 0.0808
Epoch 9/100
8075/8075 [==============================] - 437s 52ms/step - loss: 0.0798 - val_loss: 0.0796
Epoch 10/100
8075/8075 [==============================] - 437s 52ms/step - loss: 0.0796 - val_loss: 0.0801
Epoch 11/100
8075/8075 [==============================] - 439s 53ms/step - loss: 0.0794 - val_loss: 0.0796
Epoch 12/100
8075/8075 [==============================] - 438s 52ms/step - loss: 0.0793 - val_loss: 0.0798
Epoch 13/100
8075/8075 [==============================] - 440s 52ms/step - loss: 0.0791 - val_loss: 0.0801
Epoch 14/100
8075/8075 [==============================] - 438s 53ms/step - loss: 0.0790 - val_loss: 0.0798

Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.
Epoch 15/100
8075/8075 [==============================] - 439s 52ms/step - loss: 0.0785 - val_loss: 0.0789
Epoch 16/100
8075/8075 [==============================] - 440s 52ms/step - loss: 0.0783 - val_loss: 0.0789
Epoch 17/100
8075/8075 [==============================] - 437s 52ms/step - loss: 0.0782 - val_loss: 0.0789
Epoch 18/100
8075/8075 [==============================] - 437s 52ms/step - loss: 0.0781 - val_loss: 0.0789
Epoch 19/100
8075/8075 [==============================] - 438s 53ms/step - loss: 0.0780 - val_loss: 0.0789
Epoch 20/100
8075/8075 [==============================] - 438s 53ms/step - loss: 0.0780 - val_loss: 0.0789

Epoch 00020: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.
Epoch 21/100
8075/8075 [==============================] - 437s 52ms/step - loss: 0.0778 - val_loss: 0.0788
Epoch 22/100
8075/8075 [==============================] - 437s 52ms/step - loss: 0.0778 - val_loss: 0.0789
Restoring model weights from the end of the best epoch.
Epoch 00022: early stopping
Test loss: 0.078922875
Inference time: 92.85915851593018 (divide by 2 700 000 jets)
